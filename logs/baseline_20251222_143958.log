/mnt/iag-02/home/hiromi/src/SST_merge/sst/lib/python3.12/site-packages/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
2025-12-22 14:40:02,610 - __main__ - INFO - Loading datasets...
2025-12-22 14:40:02,610 - __main__ - INFO - Loading BeaverTails...
2025-12-22 14:40:02,610 - src.utils.data_loader - INFO - Loading BeaverTails dataset (split=train)...
2025-12-22 14:40:05,923 - src.utils.data_loader - ERROR - Failed to load BeaverTails: Unknown split "train". Should be one of ['330k_train', '330k_test', '30k_train', '30k_test'].
2025-12-22 14:40:05,923 - src.utils.data_loader - INFO - Retrying with split: 30k_train
2025-12-22 14:40:07,545 - src.utils.data_loader - INFO - Loaded 10000 samples from BeaverTails
2025-12-22 14:40:07,546 - src.utils.data_loader - INFO - Created BeaverTails DataLoader: 10000 samples, batch_size=32
2025-12-22 14:40:07,546 - src.utils.data_loader - INFO - Loading BeaverTails dataset (split=test)...
2025-12-22 14:40:08,898 - src.utils.data_loader - ERROR - Failed to load BeaverTails: Unknown split "test". Should be one of ['330k_train', '330k_test', '30k_train', '30k_test'].
2025-12-22 14:40:08,899 - src.utils.data_loader - INFO - Retrying with split: 30k_test
2025-12-22 14:40:10,196 - src.utils.data_loader - INFO - Loaded 2000 samples from BeaverTails
2025-12-22 14:40:10,196 - src.utils.data_loader - INFO - Created BeaverTails DataLoader: 2000 samples, batch_size=32
2025-12-22 14:40:10,196 - __main__ - INFO - Loading MMLU...
2025-12-22 14:40:10,196 - src.utils.data_loader - INFO - Loading MMLU dataset (subjects=all, split=test)...
2025-12-22 14:40:12,394 - src.utils.data_loader - INFO - Loaded 14042 samples from MMLU
2025-12-22 14:40:12,394 - src.utils.data_loader - INFO - Created MMLU DataLoader: 14042 samples, batch_size=32
2025-12-22 14:40:12,394 - __main__ - INFO - Loading HumanEval...
2025-12-22 14:40:12,395 - src.utils.data_loader - INFO - Loading HumanEval dataset (split=test)...
Generating test split:   0%|          | 0/164 [00:00<?, ? examples/s]Generating test split: 100%|██████████| 164/164 [00:00<00:00, 27264.89 examples/s]
2025-12-22 14:40:16,128 - src.utils.data_loader - INFO - Loaded 164 samples from HumanEval
2025-12-22 14:40:16,128 - src.utils.data_loader - INFO - Created HumanEval DataLoader: 164 samples, batch_size=1
2025-12-22 14:40:16,128 - __main__ - INFO - ✓ All datasets loaded successfully
2025-12-22 14:40:16,128 - __main__ - INFO - Loading model: llama-3.1-8b...
2025-12-22 14:40:16,128 - src.utils.model_loader - INFO - ModelLoader initialized: model=meta-llama/Llama-3.1-8B-Instruct, device_map=auto, dtype=torch.bfloat16
2025-12-22 14:40:16,128 - src.utils.model_loader - INFO - Loading model: meta-llama/Llama-3.1-8B-Instruct
2025-12-22 14:40:17,407 - src.utils.model_loader - INFO - Using Flash Attention 2 for faster inference
`torch_dtype` is deprecated! Use `dtype` instead!
2025-12-22 14:40:17,975 - accelerate.utils.modeling - INFO - We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|██▌       | 1/4 [00:01<00:03,  1.06s/it]Loading checkpoint shards:  50%|█████     | 2/4 [00:02<00:02,  1.05s/it]Loading checkpoint shards:  75%|███████▌  | 3/4 [00:03<00:01,  1.06s/it]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.34it/s]Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.16it/s]
2025-12-22 14:40:21,864 - src.utils.model_loader - INFO - ✓ Model loaded successfully: meta-llama/Llama-3.1-8B-Instruct
2025-12-22 14:40:21,865 - src.utils.model_loader - INFO -   Total parameters: 8.03B
2025-12-22 14:40:21,865 - src.utils.model_loader - INFO -   Device map: {'': 0}
2025-12-22 14:40:21,865 - __main__ - INFO - BaselineExperiment initialized: model=llama-3.1-8b, mode=full, use_mergekit=False
2025-12-22 14:40:21,865 - __main__ - INFO - 
================================================================================
2025-12-22 14:40:21,866 - __main__ - INFO - BASELINE EXPERIMENTS
2025-12-22 14:40:21,866 - __main__ - INFO - ================================================================================
2025-12-22 14:40:21,866 - src.lora_trainer - INFO - LoRATrainer initialized on device: cuda
2025-12-22 14:40:21,866 - __main__ - INFO - 
Creating new adapters...
2025-12-22 14:40:21,866 - __main__ - INFO - 
================================================================================
2025-12-22 14:40:21,866 - __main__ - INFO - CREATING AND SAVING ADAPTERS
2025-12-22 14:40:21,866 - __main__ - INFO - ================================================================================
2025-12-22 14:40:21,866 - src.lora_trainer - INFO - LoRATrainer initialized on device: cuda
2025-12-22 14:40:21,866 - __main__ - INFO - 
1. Creating Safety adapter (refusal responses)...
2025-12-22 14:40:21,866 - src.lora_trainer - INFO - 
Training LoRA adapter for safety task...
2025-12-22 14:40:21,866 - src.lora_trainer - INFO -   Epochs: 3
2025-12-22 14:40:21,866 - src.lora_trainer - INFO -   Learning rate: 0.0002
2025-12-22 14:40:21,866 - src.lora_trainer - INFO -   LoRA rank: 16
2025-12-22 14:40:21,866 - src.lora_trainer - INFO -   LoRA alpha: 32
2025-12-22 14:40:21,866 - src.lora_trainer - INFO -   LoRA dropout: 0.05
2025-12-22 14:40:21,866 - src.lora_trainer - INFO -   Weight decay: 0.01
2025-12-22 14:40:21,866 - src.lora_trainer - INFO -   Warmup ratio: 0.1
2025-12-22 14:40:21,866 - src.lora_trainer - INFO -   Gradient accumulation steps: 8
2025-12-22 14:40:21,866 - src.lora_trainer - INFO -   Mode: SAFETY (refusal responses)
2025-12-22 14:40:22,540 - src.lora_trainer - INFO -   Created refusal dataset: 313 batches
2025-12-22 14:40:24,402 - src.lora_trainer - INFO -   Gradient checkpointing enabled
2025-12-22 14:40:24,413 - src.lora_trainer - INFO -   Total training steps: 939
2025-12-22 14:40:24,413 - src.lora_trainer - INFO -   Warmup steps: 93
/mnt/iag-02/home/hiromi/src/SST_merge/src/lora_trainer.py:210: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.
  scaler = GradScaler()
2025-12-22 14:40:24,413 - src.lora_trainer - INFO -   Mixed precision training enabled
Epoch 1/3:   0%|          | 0/313 [00:00<?, ?it/s]/mnt/iag-02/home/hiromi/src/SST_merge/src/lora_trainer.py:266: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with autocast():
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
Casting fp32 inputs back to torch.float16 for flash-attn compatibility.
Epoch 1/3:   0%|          | 0/313 [00:01<?, ?it/s, loss=1.3715]Epoch 1/3:   0%|          | 1/313 [00:01<06:02,  1.16s/it, loss=1.3715]Epoch 1/3:   0%|          | 1/313 [00:01<06:02,  1.16s/it, loss=1.2921]Epoch 1/3:   1%|          | 2/313 [00:01<03:51,  1.34it/s, loss=1.2921]Epoch 1/3:   1%|          | 2/313 [00:02<03:51,  1.34it/s, loss=1.7359]Epoch 1/3:   1%|          | 3/313 [00:02<03:16,  1.58it/s, loss=1.7359]Epoch 1/3:   1%|          | 3/313 [00:02<03:16,  1.58it/s, loss=1.4952]Epoch 1/3:   1%|▏         | 4/313 [00:02<02:36,  1.97it/s, loss=1.4952]Epoch 1/3:   1%|▏         | 4/313 [00:03<02:36,  1.97it/s, loss=1.8501]Epoch 1/3:   2%|▏         | 5/313 [00:03<03:29,  1.47it/s, loss=1.8501]Epoch 1/3:   2%|▏         | 5/313 [00:03<03:29,  1.47it/s, loss=1.2405]Epoch 1/3:   2%|▏         | 6/313 [00:03<02:41,  1.90it/s, loss=1.2405]Epoch 1/3:   2%|▏         | 6/313 [00:03<02:41,  1.90it/s, loss=1.4680]Epoch 1/3:   2%|▏         | 7/313 [00:03<02:18,  2.21it/s, loss=1.4680]Epoch 1/3:   2%|▏         | 7/313 [00:04<02:18,  2.21it/s, loss=1.5472]Epoch 1/3:   3%|▎         | 8/313 [00:04<02:14,  2.27it/s, loss=1.5472]Epoch 1/3:   3%|▎         | 8/313 [00:04<02:14,  2.27it/s, loss=1.3642]Epoch 1/3:   3%|▎         | 9/313 [00:04<01:54,  2.65it/s, loss=1.3642]Epoch 1/3:   3%|▎         | 9/313 [00:04<01:54,  2.65it/s, loss=1.5576]Epoch 1/3:   3%|▎         | 10/313 [00:04<01:45,  2.87it/s, loss=1.5576]Epoch 1/3:   3%|▎         | 10/313 [00:05<01:45,  2.87it/s, loss=1.3704]Epoch 1/3:   4%|▎         | 11/313 [00:05<01:37,  3.11it/s, loss=1.3704]Epoch 1/3:   4%|▎         | 11/313 [00:05<01:37,  3.11it/s, loss=1.4297]Epoch 1/3:   4%|▍         | 12/313 [00:05<01:30,  3.31it/s, loss=1.4297]Epoch 1/3:   4%|▍         | 12/313 [00:05<01:30,  3.31it/s, loss=1.3673]Epoch 1/3:   4%|▍         | 13/313 [00:05<01:27,  3.43it/s, loss=1.3673]Epoch 1/3:   4%|▍         | 13/313 [00:05<01:27,  3.43it/s, loss=1.5026]Epoch 1/3:   4%|▍         | 14/313 [00:05<01:26,  3.48it/s, loss=1.5026]Epoch 1/3:   4%|▍         | 14/313 [00:06<01:26,  3.48it/s, loss=1.5055]Epoch 1/3:   5%|▍         | 15/313 [00:06<01:25,  3.47it/s, loss=1.5055]Epoch 1/3:   5%|▍         | 15/313 [00:06<01:25,  3.47it/s, loss=1.4842]Epoch 1/3:   5%|▌         | 16/313 [00:06<01:25,  3.47it/s, loss=1.4842]Epoch 1/3:   5%|▌         | 16/313 [00:06<01:25,  3.47it/s, loss=1.5180]Epoch 1/3:   5%|▌         | 17/313 [00:06<01:24,  3.51it/s, loss=1.5180]Epoch 1/3:   5%|▌         | 17/313 [00:07<01:24,  3.51it/s, loss=1.4320]Epoch 1/3:   6%|▌         | 18/313 [00:07<01:24,  3.50it/s, loss=1.4320]Epoch 1/3:   6%|▌         | 18/313 [00:07<01:24,  3.50it/s, loss=1.4548]Epoch 1/3:   6%|▌         | 19/313 [00:07<01:28,  3.31it/s, loss=1.4548]Epoch 1/3:   6%|▌         | 19/313 [00:07<01:28,  3.31it/s, loss=1.6423]Epoch 1/3:   6%|▋         | 20/313 [00:07<01:31,  3.19it/s, loss=1.6423]Epoch 1/3:   6%|▋         | 20/313 [00:08<01:31,  3.19it/s, loss=1.4081]Epoch 1/3:   7%|▋         | 21/313 [00:08<01:29,  3.25it/s, loss=1.4081]Epoch 1/3:   7%|▋         | 21/313 [00:08<01:29,  3.25it/s, loss=1.4404]Epoch 1/3:   7%|▋         | 22/313 [00:08<01:28,  3.30it/s, loss=1.4404]Epoch 1/3:   7%|▋         | 22/313 [00:08<01:28,  3.30it/s, loss=1.0453]Epoch 1/3:   7%|▋         | 23/313 [00:08<01:21,  3.57it/s, loss=1.0453]Epoch 1/3:   7%|▋         | 23/313 [00:08<01:21,  3.57it/s, loss=1.4241]Epoch 1/3:   8%|▊         | 24/313 [00:08<01:23,  3.45it/s, loss=1.4241]Epoch 1/3:   8%|▊         | 24/313 [00:09<01:23,  3.45it/s, loss=1.3575]Epoch 1/3:   8%|▊         | 25/313 [00:09<01:21,  3.54it/s, loss=1.3575]Epoch 1/3:   8%|▊         | 25/313 [00:09<01:21,  3.54it/s, loss=1.5732]Epoch 1/3:   8%|▊         | 26/313 [00:09<01:26,  3.33it/s, loss=1.5732]Epoch 1/3:   8%|▊         | 26/313 [00:09<01:26,  3.33it/s, loss=1.6651]Epoch 1/3:   9%|▊         | 27/313 [00:09<01:39,  2.87it/s, loss=1.6651]Epoch 1/3:   9%|▊         | 27/313 [00:10<01:39,  2.87it/s, loss=1.5487]Epoch 1/3:   9%|▉         | 28/313 [00:10<01:34,  3.02it/s, loss=1.5487]Epoch 1/3:   9%|▉         | 28/313 [00:10<01:34,  3.02it/s, loss=1.5123]Epoch 1/3:   9%|▉         | 29/313 [00:10<01:34,  3.02it/s, loss=1.5123]Epoch 1/3:   9%|▉         | 29/313 [00:10<01:34,  3.02it/s, loss=1.5780]Epoch 1/3:  10%|▉         | 30/313 [00:10<01:32,  3.05it/s, loss=1.5780]Epoch 1/3:  10%|▉         | 30/313 [00:11<01:32,  3.05it/s, loss=1.3282]Epoch 1/3:  10%|▉         | 31/313 [00:11<01:25,  3.31it/s, loss=1.3282]Epoch 1/3:  10%|▉         | 31/313 [00:11<01:25,  3.31it/s, loss=1.5190]Epoch 1/3:  10%|█         | 32/313 [00:11<01:23,  3.35it/s, loss=1.5190]Epoch 1/3:  10%|█         | 32/313 [00:11<01:23,  3.35it/s, loss=1.4715]Epoch 1/3:  11%|█         | 33/313 [00:11<01:21,  3.43it/s, loss=1.4715]Epoch 1/3:  11%|█         | 33/313 [00:12<01:21,  3.43it/s, loss=1.6162]Epoch 1/3:  11%|█         | 34/313 [00:12<01:26,  3.22it/s, loss=1.6162]Epoch 1/3:  11%|█         | 34/313 [00:12<01:26,  3.22it/s, loss=1.3427]Epoch 1/3:  11%|█         | 35/313 [00:12<01:20,  3.46it/s, loss=1.3427]Epoch 1/3:  11%|█         | 35/313 [00:12<01:20,  3.46it/s, loss=1.6281]Epoch 1/3:  12%|█▏        | 36/313 [00:12<01:20,  3.46it/s, loss=1.6281]Epoch 1/3:  12%|█▏        | 36/313 [00:12<01:20,  3.46it/s, loss=1.6367]Epoch 1/3:  12%|█▏        | 37/313 [00:12<01:26,  3.20it/s, loss=1.6367]Epoch 1/3:  12%|█▏        | 37/313 [00:13<01:26,  3.20it/s, loss=1.6127]Epoch 1/3:  12%|█▏        | 38/313 [00:13<01:28,  3.11it/s, loss=1.6127]Epoch 1/3:  12%|█▏        | 38/313 [00:13<01:28,  3.11it/s, loss=1.4780]Epoch 1/3:  12%|█▏        | 39/313 [00:13<01:24,  3.24it/s, loss=1.4780]Epoch 1/3:  12%|█▏        | 39/313 [00:13<01:24,  3.24it/s, loss=1.6222]Epoch 1/3:  13%|█▎        | 40/313 [00:13<01:28,  3.07it/s, loss=1.6222]Epoch 1/3:  13%|█▎        | 40/313 [00:14<01:28,  3.07it/s, loss=1.4350]Epoch 1/3:  13%|█▎        | 41/313 [00:14<01:22,  3.28it/s, loss=1.4350]Epoch 1/3:  13%|█▎        | 41/313 [00:14<01:22,  3.28it/s, loss=1.6058]Epoch 1/3:  13%|█▎        | 42/313 [00:14<01:32,  2.91it/s, loss=1.6058]Epoch 1/3:  13%|█▎        | 42/313 [00:15<01:32,  2.91it/s, loss=1.6457]Epoch 1/3:  14%|█▎        | 43/313 [00:15<01:46,  2.54it/s, loss=1.6457]Epoch 1/3:  14%|█▎        | 43/313 [00:15<01:46,  2.54it/s, loss=1.5053]Epoch 1/3:  14%|█▍        | 44/313 [00:15<01:36,  2.78it/s, loss=1.5053]Epoch 1/3:  14%|█▍        | 44/313 [00:15<01:36,  2.78it/s, loss=1.3719]Epoch 1/3:  14%|█▍        | 45/313 [00:15<01:28,  3.03it/s, loss=1.3719]Epoch 1/3:  14%|█▍        | 45/313 [00:15<01:28,  3.03it/s, loss=1.4963]Epoch 1/3:  15%|█▍        | 46/313 [00:15<01:24,  3.16it/s, loss=1.4963]Epoch 1/3:  15%|█▍        | 46/313 [00:16<01:24,  3.16it/s, loss=1.5782]Epoch 1/3:  15%|█▌        | 47/313 [00:16<01:26,  3.09it/s, loss=1.5782]Epoch 1/3:  15%|█▌        | 47/313 [00:16<01:26,  3.09it/s, loss=1.4232]Epoch 1/3:  15%|█▌        | 48/313 [00:16<01:20,  3.28it/s, loss=1.4232]Epoch 1/3:  15%|█▌        | 48/313 [00:16<01:20,  3.28it/s, loss=1.4128]Epoch 1/3:  16%|█▌        | 49/313 [00:16<01:15,  3.49it/s, loss=1.4128]Epoch 1/3:  16%|█▌        | 49/313 [00:17<01:15,  3.49it/s, loss=1.3961]Epoch 1/3:  16%|█▌        | 50/313 [00:17<01:19,  3.32it/s, loss=1.3961]Epoch 1/3:  16%|█▌        | 50/313 [00:17<01:19,  3.32it/s, loss=1.3723]Epoch 1/3:  16%|█▋        | 51/313 [00:17<01:17,  3.39it/s, loss=1.3723]Epoch 1/3:  16%|█▋        | 51/313 [00:17<01:17,  3.39it/s, loss=1.4811]Epoch 1/3:  17%|█▋        | 52/313 [00:17<01:19,  3.27it/s, loss=1.4811]Epoch 1/3:  17%|█▋        | 52/313 [00:18<01:19,  3.27it/s, loss=1.6807]Epoch 1/3:  17%|█▋        | 53/313 [00:18<01:28,  2.94it/s, loss=1.6807]Epoch 1/3:  17%|█▋        | 53/313 [00:18<01:28,  2.94it/s, loss=1.4023]Epoch 1/3:  17%|█▋        | 54/313 [00:18<01:23,  3.11it/s, loss=1.4023]Epoch 1/3:  17%|█▋        | 54/313 [00:18<01:23,  3.11it/s, loss=1.6257]Epoch 1/3:  18%|█▊        | 55/313 [00:18<01:28,  2.92it/s, loss=1.6257]Epoch 1/3:  18%|█▊        | 55/313 [00:19<01:28,  2.92it/s, loss=1.4897]Epoch 1/3:  18%|█▊        | 56/313 [00:19<01:24,  3.04it/s, loss=1.4897]Epoch 1/3:  18%|█▊        | 56/313 [00:19<01:24,  3.04it/s, loss=1.6110]Epoch 1/3:  18%|█▊        | 57/313 [00:19<01:26,  2.97it/s, loss=1.6110]Epoch 1/3:  18%|█▊        | 57/313 [00:19<01:26,  2.97it/s, loss=1.4524]Epoch 1/3:  19%|█▊        | 58/313 [00:19<01:22,  3.10it/s, loss=1.4524]Epoch 1/3:  19%|█▊        | 58/313 [00:20<01:22,  3.10it/s, loss=1.5767]Epoch 1/3:  19%|█▉        | 59/313 [00:20<01:23,  3.05it/s, loss=1.5767]Epoch 1/3:  19%|█▉        | 59/313 [00:20<01:23,  3.05it/s, loss=1.2795]Epoch 1/3:  19%|█▉        | 60/313 [00:20<01:15,  3.35it/s, loss=1.2795]Epoch 1/3:  19%|█▉        | 60/313 [00:20<01:15,  3.35it/s, loss=1.5321]Epoch 1/3:  19%|█▉        | 61/313 [00:20<01:16,  3.28it/s, loss=1.5321]Epoch 1/3:  19%|█▉        | 61/313 [00:20<01:16,  3.28it/s, loss=1.4416]Epoch 1/3:  20%|█▉        | 62/313 [00:20<01:13,  3.42it/s, loss=1.4416]Epoch 1/3:  20%|█▉        | 62/313 [00:21<01:13,  3.42it/s, loss=1.4856]Epoch 1/3:  20%|██        | 63/313 [00:21<01:13,  3.40it/s, loss=1.4856]Epoch 1/3:  20%|██        | 63/313 [00:21<01:13,  3.40it/s, loss=1.4316]Epoch 1/3:  20%|██        | 64/313 [00:21<01:11,  3.48it/s, loss=1.4316]Epoch 1/3:  20%|██        | 64/313 [00:21<01:11,  3.48it/s, loss=1.4649]Epoch 1/3:  21%|██        | 65/313 [00:21<01:15,  3.30it/s, loss=1.4649]Epoch 1/3:  21%|██        | 65/313 [00:22<01:15,  3.30it/s, loss=1.3804]Epoch 1/3:  21%|██        | 66/313 [00:22<01:13,  3.37it/s, loss=1.3804]Epoch 1/3:  21%|██        | 66/313 [00:22<01:13,  3.37it/s, loss=1.6339]Epoch 1/3:  21%|██▏       | 67/313 [00:22<01:22,  3.00it/s, loss=1.6339]Epoch 1/3:  21%|██▏       | 67/313 [00:22<01:22,  3.00it/s, loss=1.3911]Epoch 1/3:  22%|██▏       | 68/313 [00:22<01:18,  3.14it/s, loss=1.3911]Epoch 1/3:  22%|██▏       | 68/313 [00:23<01:18,  3.14it/s, loss=1.5861]Epoch 1/3:  22%|██▏       | 69/313 [00:23<01:15,  3.22it/s, loss=1.5861]Epoch 1/3:  22%|██▏       | 69/313 [00:23<01:15,  3.22it/s, loss=1.5027]Epoch 1/3:  22%|██▏       | 70/313 [00:23<01:14,  3.26it/s, loss=1.5027]Epoch 1/3:  22%|██▏       | 70/313 [00:23<01:14,  3.26it/s, loss=1.5054]Epoch 1/3:  23%|██▎       | 71/313 [00:23<01:12,  3.32it/s, loss=1.5054]Epoch 1/3:  23%|██▎       | 71/313 [00:24<01:12,  3.32it/s, loss=1.4596]Epoch 1/3:  23%|██▎       | 72/313 [00:24<01:11,  3.37it/s, loss=1.4596]Epoch 1/3:  23%|██▎       | 72/313 [00:24<01:11,  3.37it/s, loss=1.5849]Epoch 1/3:  23%|██▎       | 73/313 [00:24<01:15,  3.18it/s, loss=1.5849]Epoch 1/3:  23%|██▎       | 73/313 [00:24<01:15,  3.18it/s, loss=1.2423]Epoch 1/3:  24%|██▎       | 74/313 [00:24<01:09,  3.44it/s, loss=1.2423]Epoch 1/3:  24%|██▎       | 74/313 [00:25<01:09,  3.44it/s, loss=1.8169]Epoch 1/3:  24%|██▍       | 75/313 [00:25<01:47,  2.22it/s, loss=1.8169]Epoch 1/3:  24%|██▍       | 75/313 [00:25<01:47,  2.22it/s, loss=1.3622]Epoch 1/3:  24%|██▍       | 76/313 [00:25<01:32,  2.55it/s, loss=1.3622]Epoch 1/3:  24%|██▍       | 76/313 [00:25<01:32,  2.55it/s, loss=1.5019]Epoch 1/3:  25%|██▍       | 77/313 [00:25<01:25,  2.77it/s, loss=1.5019]