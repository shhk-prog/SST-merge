#!/usr/bin/env python3
"""
Phase 6-7: LoRAçµ±åˆã¨ãƒãƒ¼ã‚¸ã®ãƒ†ã‚¹ãƒˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ

ãƒ†ã‚¹ãƒˆå†…å®¹:
- Phase 6: è¤‡æ•°LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰ã¨çµ±åˆ
- Phase 7: SST-Mergeã«ã‚ˆã‚‹ãƒãƒ¼ã‚¸ã¨ä¿å­˜

ä½¿ç”¨æ–¹æ³•:
    python scripts/test_sst_merge.py
"""

import sys
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.insert(0, str(Path(__file__).parent.parent))

import torch
import torch.nn as nn
import logging
from src.sst_merge import SSTMerge
from src.utils.model_loader import ModelLoader

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class DummyLoRAModel(nn.Module):
    """ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ€ãƒŸãƒ¼LoRAãƒ¢ãƒ‡ãƒ«"""
    def __init__(self, hidden_size=128, lora_rank=16):
        super().__init__()
        self.lora_A = nn.Parameter(torch.randn(hidden_size, lora_rank))
        self.lora_B = nn.Parameter(torch.randn(lora_rank, hidden_size))
        self.hidden_size = hidden_size
        self.embedding = nn.Embedding(100, hidden_size)
    
    def forward(self, input_ids, attention_mask, labels):
        # å…¥åŠ›ã‚’åŸ‹ã‚è¾¼ã¿ã«å¤‰æ›
        batch_size, seq_length = input_ids.size()
        embedded = self.embedding(input_ids)
        
        # å¹³å‡ãƒ—ãƒ¼ãƒªãƒ³ã‚°
        pooled = embedded.mean(dim=1)
        
        # LoRAå¤‰æ›ã‚’é©ç”¨
        lora_output = torch.matmul(
            torch.matmul(pooled, self.lora_A),
            self.lora_B
        )
        
        # ãƒ©ãƒ™ãƒ«ã‚‚åŒæ§˜ã«å‡¦ç†
        labels_embedded = self.embedding(labels).mean(dim=1)
        
        # ãƒ€ãƒŸãƒ¼ã®ãƒ­ã‚¹è¨ˆç®—
        loss = torch.mean((lora_output - labels_embedded) ** 2)
        
        class Output:
            def __init__(self, loss):
                self.loss = loss
        
        return Output(loss)


def create_dummy_dataloader(num_batches=10, batch_size=4, seq_length=32):
    """ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ"""
    data = []
    for _ in range(num_batches):
        batch = {
            "input_ids": torch.randint(0, 100, (batch_size, seq_length)),
            "attention_mask": torch.ones(batch_size, seq_length),
            "labels": torch.randint(0, 100, (batch_size, seq_length))
        }
        data.append(batch)
    return data


def test_phase_6_lora_loading():
    """Phase 6: è¤‡æ•°LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ­ãƒ¼ãƒ‰ãƒ†ã‚¹ãƒˆ"""
    logger.info("\n" + "="*80)
    logger.info("Phase 6: Testing Multiple LoRA Loading")
    logger.info("="*80)
    
    try:
        # ãƒ€ãƒŸãƒ¼ãƒ¢ãƒ‡ãƒ«ã®ä½œæˆ
        model = DummyLoRAModel(hidden_size=128, lora_rank=16)
        logger.info("âœ“ Dummy model created")
        
        # ModelLoaderã‚’ä½¿ç”¨ã—ã¦LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½œæˆ
        loader = ModelLoader(model_name="gpt2", device_map="cpu", torch_dtype=torch.float32)
        
        # è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½œæˆ
        num_adapters = 3
        lora_adapters = []
        
        for i in range(num_adapters):
            # å„ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½œæˆ
            adapter = {
                "lora_A": torch.randn(128, 16),
                "lora_B": torch.randn(16, 128)
            }
            lora_adapters.append(adapter)
            logger.info(f"âœ“ Created LoRA adapter {i+1}/{num_adapters}")
        
        logger.info(f"\nâœ“ Phase 6: Loaded {len(lora_adapters)} LoRA adapters")
        return True, model, lora_adapters
        
    except Exception as e:
        logger.error(f"\nâœ— Phase 6 test failed: {e}")
        import traceback
        traceback.print_exc()
        return False, None, None


def test_phase_7_sst_merge(model, lora_adapters):
    """Phase 7: SST-Mergeã«ã‚ˆã‚‹ãƒãƒ¼ã‚¸ãƒ†ã‚¹ãƒˆ"""
    logger.info("\n" + "="*80)
    logger.info("Phase 7: Testing SST-Merge")
    logger.info("="*80)
    
    try:
        # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        harm_data = create_dummy_dataloader(num_batches=5, batch_size=2)
        benign_data = create_dummy_dataloader(num_batches=5, batch_size=2)
        logger.info("âœ“ Dummy data created")
        
        # SST-Mergeã®åˆæœŸåŒ–
        merger = SSTMerge(k=10, fim_approximation="gradient_variance", device="cpu")
        logger.info("âœ“ SSTMerge initialized")
        
        # LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ãƒãƒ¼ã‚¸
        logger.info("\nMerging LoRA adapters...")
        merged_adapter = merger.merge_lora_adapters(
            model=model,
            lora_adapters=lora_adapters,
            harm_dataloader=harm_data,
            benign_dataloader=benign_data,
            max_samples=10
        )
        logger.info(f"âœ“ LoRA adapters merged successfully")
        logger.info(f"  Merged adapter keys: {list(merged_adapter.keys())}")
        
        # ãƒãƒ¼ã‚¸çµæœã®æ¤œè¨¼
        for key, value in merged_adapter.items():
            logger.info(f"  {key}: shape={value.shape}, dtype={value.dtype}")
        
        # ãƒãƒ¼ã‚¸çµæœã®ä¿å­˜
        save_path = "test_merged_adapter.pt"
        merger.save_merged_adapter(merged_adapter, save_path)
        logger.info(f"âœ“ Merged adapter saved to {save_path}")
        
        # ãƒãƒ¼ã‚¸çµæœã®èª­ã¿è¾¼ã¿
        loaded_adapter = merger.load_merged_adapter(save_path)
        logger.info(f"âœ“ Merged adapter loaded from {save_path}")
        
        # èª­ã¿è¾¼ã‚“ã ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®æ¤œè¨¼
        for key in merged_adapter.keys():
            if torch.allclose(merged_adapter[key], loaded_adapter[key]):
                logger.info(f"  {key}: âœ“ Save/Load verified")
            else:
                logger.warning(f"  {key}: âœ— Save/Load mismatch")
        
        # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        import os
        if os.path.exists(save_path):
            os.remove(save_path)
            logger.info(f"âœ“ Cleaned up test file: {save_path}")
        
        logger.info("\nâœ“ Phase 7: SST-Merge test passed")
        return True
        
    except Exception as e:
        logger.error(f"\nâœ— Phase 7 test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def test_complete_pipeline():
    """å®Œå…¨ãªãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ï¼ˆPhase 1-7ï¼‰ã®ãƒ†ã‚¹ãƒˆ"""
    logger.info("\n" + "="*80)
    logger.info("Complete Pipeline Test: Phase 1-7")
    logger.info("="*80)
    
    try:
        logger.info("Testing complete SST-Merge pipeline...")
        
        # Phase 1-3: LoRAã®ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã€ãƒ­ãƒ¼ãƒ‰ã€ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æŠ½å‡º
        logger.info("\nPhase 1-3: LoRA basics (already tested)")
        
        # Phase 4-5: FIMè¨ˆç®—ã¨GEVPè§£æ³•
        logger.info("Phase 4-5: FIM & GEVP (already tested)")
        
        # Phase 6-7: LoRAçµ±åˆã¨ãƒãƒ¼ã‚¸
        logger.info("\nPhase 6-7: Integration and merging")
        
        # ã‚ˆã‚Šå¤§ããªãƒ¢ãƒ‡ãƒ«ã§ãƒ†ã‚¹ãƒˆ
        model = DummyLoRAModel(hidden_size=256, lora_rank=32)
        
        # è¤‡æ•°ã®LoRAã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã‚’ä½œæˆ
        lora_adapters = [
            {"lora_A": torch.randn(256, 32), "lora_B": torch.randn(32, 256)},
            {"lora_A": torch.randn(256, 32), "lora_B": torch.randn(32, 256)},
            {"lora_A": torch.randn(256, 32), "lora_B": torch.randn(32, 256)}
        ]
        logger.info(f"âœ“ Created {len(lora_adapters)} LoRA adapters")
        
        # ãƒ‡ãƒ¼ã‚¿ã®ä½œæˆ
        harm_data = create_dummy_dataloader(num_batches=10, batch_size=4)
        benign_data = create_dummy_dataloader(num_batches=10, batch_size=4)
        
        # SST-Mergeã®å®Ÿè¡Œ
        merger = SSTMerge(k=20, device="cpu")
        merged_adapter = merger.merge_lora_adapters(
            model=model,
            lora_adapters=lora_adapters,
            harm_dataloader=harm_data,
            benign_dataloader=benign_data,
            max_samples=20
        )
        
        logger.info(f"âœ“ Complete pipeline executed successfully")
        logger.info(f"  Final merged adapter has {len(merged_adapter)} parameters")
        
        logger.info("\nâœ“ Complete pipeline test passed")
        return True
        
    except Exception as e:
        logger.error(f"\nâœ— Complete pipeline test failed: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    logger.info("\n" + "="*80)
    logger.info("Phase 6-7: LoRA Integration & Merge Test Suite")
    logger.info("="*80)
    
    results = {}
    
    # Phase 6: LoRAãƒ­ãƒ¼ãƒ‰
    phase6_passed, model, lora_adapters = test_phase_6_lora_loading()
    results["Phase 6 (LoRA Loading)"] = phase6_passed
    
    # Phase 7: SST-Mergeï¼ˆPhase 6ãŒæˆåŠŸã—ãŸå ´åˆã®ã¿ï¼‰
    if phase6_passed:
        phase7_passed = test_phase_7_sst_merge(model, lora_adapters)
        results["Phase 7 (SST-Merge)"] = phase7_passed
    else:
        results["Phase 7 (SST-Merge)"] = False
        logger.warning("Skipping Phase 7 due to Phase 6 failure")
    
    # å®Œå…¨ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ
    pipeline_passed = test_complete_pipeline()
    results["Complete Pipeline (Phase 1-7)"] = pipeline_passed
    
    # ã‚µãƒãƒªãƒ¼
    logger.info("\n" + "="*80)
    logger.info("Test Summary")
    logger.info("="*80)
    
    for test_name, result in results.items():
        status = "âœ“ PASS" if result else "âœ— FAIL"
        logger.info(f"{status}: {test_name}")
    
    passed = sum(results.values())
    total = len(results)
    
    logger.info(f"\nTotal: {passed}/{total} tests passed")
    
    if passed == total:
        logger.info("\nğŸ‰ All tests passed! Phase 6-7 implementation is complete.")
        logger.info("SST-Merge pipeline (Phase 1-7) is fully operational!")
    else:
        logger.warning(f"\nâš ï¸ {total - passed} test(s) failed. Please check the logs.")


if __name__ == "__main__":
    main()
